{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement learning on PyTorch to solve Open AI Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v1').unwrapped\n",
    "\n",
    "# setting matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_python:\n",
    "    from Ipython import Display\n",
    "    \n",
    "# check if gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay memory will be used for training our DQN network which helps in storing the transitions the agent undergoes allowing us to make use of this data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('transition',('state','action','next_state','reward'))\n",
    "\n",
    "class ReplayMem(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, *args):\n",
    "        ## Save a transition\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Neural Network Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, h, w, out):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.total_actions = 3\n",
    "        self.gamma = 0.99\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.initial_epsilon = 0.1\n",
    "        self.num_iterations = 2000000\n",
    "        self.replay_mem_size = 10000\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 5, stride = 2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size = 5, stride = 2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size = 5, stride = 2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # compute the number of linear input connections\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size,out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.compose([T.ToPILImage(),\n",
    "                   T.Resize(40, interpolation = Image.CUBIC),\n",
    "                   T.ToTensor()])\n",
    "\n",
    "def capt_screen():\n",
    "    screen = env.render(mode = 'rgb_array').transpose(2,0,1)\n",
    "    \n",
    "    # convert to float, rescale and convert to torch tensor\n",
    "    screen = np.ascontiguousarray(screen, dtype = np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1,2,0).numpy(),interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get the screen size so that we can initialize layers correctly based on what we get from AI gym\n",
    "ini_screen = capt_screen()\n",
    "_, _, screen_height, screen_width = ini_screen.shape\n",
    "\n",
    "# get total possible actions from gym action space\n",
    "tot_actions = env.action_space.n\n",
    "\n",
    "policy_net = Network(screen_height, screen_width, tot_actions).to(device)\n",
    "target_net = Network(screen_height, screen_width, tot_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device = device, dtype=torch.long)\n",
    "    \n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype = torch.float)\n",
    "    plt.title('Training')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99),means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Transposing the batch which converts a batch-array of transitions to a transition of batch array\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Determine mask of non-final states and concatenate batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    \n",
    "    non_final_next_states = torch.cat([ s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Now compute Q(s_t,a) by calculating Q(s_t) from the output of the model and selecting the action which we would take according to the present state from policy net\n",
    "    state_action_val = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Determine expected values of action for non final next states which is V(s_{t+1}) based on old target net\n",
    "    next_state_val = torch.zeros(BATCH_SIZES, device = device)\n",
    "    next_state_val[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # compute expected Q values\n",
    "    expected_state_action_val = (next_state_val * GAMMA) + reward_batch\n",
    "    \n",
    "    # compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_val.unsqueeze(1))\n",
    "    \n",
    "    # optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eps = 55\n",
    "for i_episode in range(num_eps):\n",
    "    # Initiliaze the environment and state\n",
    "    env.reset()\n",
    "    last_screen = capt_screen()\n",
    "    current_screen = capt_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device = device)\n",
    "        \n",
    "        # observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = capt_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "        \n",
    "        # Store transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Perform one step of optimization\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t+1)\n",
    "            plot_durations()\n",
    "            break\n",
    "        # Update the target network\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
